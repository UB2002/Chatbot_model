{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0ecf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG_agent\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf18e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the access token\n",
    "access_token = os.getenv(\"ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952517d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(f\"tokenizer/{model_id}\")\n",
    "model.save_pretrained(f\"model/{model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_id}\", local_files_only = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"model/{model_id}\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbf18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6204a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompt\n",
    "prompt = \"what is the capital of france\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and print\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86deb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class QwenChatbot:\n",
    "    def __init__(self, model_id=\"Qwen/Qwen3-0.6B\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_id}\", local_files_only = True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(f\"model/{model_id}\", local_files_only = True)\n",
    "        self.history = []\n",
    "        \n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n",
    "        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Update history\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return response\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = QwenChatbot()\n",
    "\n",
    "    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n",
    "    user_input_1 = \"How many r's in strawberries?\"\n",
    "    print(f\"User: {user_input_1}\")\n",
    "    response_1 = chatbot.generate_response(user_input_1)\n",
    "    print(f\"Bot: {response_1}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "    # Second input with /no_think\n",
    "    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n",
    "    print(f\"User: {user_input_2}\")\n",
    "    response_2 = chatbot.generate_response(user_input_2)\n",
    "    print(f\"Bot: {response_2}\") \n",
    "    print(\"----------------------\")\n",
    "\n",
    "    # Third input with /think\n",
    "    user_input_3 = \"Really? /think\"\n",
    "    print(f\"User: {user_input_3}\")\n",
    "    response_3 = chatbot.generate_response(user_input_3)\n",
    "    print(f\"Bot: {response_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985fd6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG_agent\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List, Dict, Any, Optional\n",
    "from rag import Rag\n",
    "import torch\n",
    "class QwenRAGChatbot:\n",
    "    def __init__(self, model_id=\"Qwen/Qwen3-0.6B\", faiss_index_path=\"faiss_index\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"tokenizer/{model_id}\", local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(f\"model/{model_id}\", local_files_only=True).to(self.device)\n",
    "        self.rag = Rag(faiss_index_path=faiss_index_path)\n",
    "        self.history = []\n",
    "    \n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc, _ in docs])\n",
    "    \n",
    "    def retrieve(self, query: str) -> List:\n",
    "        return self.rag.retriever(query)\n",
    "    \n",
    "    def generate_answer(self, query: str, context: str) -> str:\n",
    "        prompt = f\"\"\"Answer the question based on the provided context. If you cannot find the answer in the context, \n",
    "say that you don't know but try to provide general information related to the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        messages = self.history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        response_ids = self.model.generate(**inputs, max_new_tokens=1024)[0][len(inputs.input_ids[0]):].tolist()\n",
    "        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Update history with the actual user query (not the augmented prompt)\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat(self, query: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            retrieved_docs = self.retrieve(query)\n",
    "            context = self.format_docs(retrieved_docs)\n",
    "            answer = self.generate_answer(query, context)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": [{\"content\": doc.page_content, \n",
    "                             \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n",
    "                             \"score\": score} \n",
    "                            for doc, score in retrieved_docs]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "            \n",
    "    def clear_history(self):\n",
    "        self.history = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9226a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chatbot = QwenRAGChatbot()\n",
    "    \n",
    "query = \"What is your product?\"\n",
    "response = chatbot.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e37f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is your product?\n",
      "\n",
      "Answer: <think>\n",
      "Okay, let me see. The user is asking for the answer to the question \"What is your product?\" based on the provided context. \n",
      "\n",
      "Looking at the documents, Document 1 is the company overview. In that document, there's a mention: \"TechTrend Innovations is a leading technology company specializing in artificial intelligence and machine learning solutions.\" So the product here is AI and machine learning solutions.\n",
      "\n",
      "Then there's Document 4, which is the product specifications for IntelliBot Chatbot. The product name is IntelliBot, version 2.1, released March 10, 2025, and the key features include AI-driven chatbots for customer service. The system requirements and pricing are listed there too. \n",
      "\n",
      "So, the product mentioned in the FAQs is the IntelliBot chatbot. The answer should be that the product is IntelliBot, which is an AI-driven chatbot. \n",
      "\n",
      "I need to make sure there's no conflicting information. The FAQs section only includes the question and the answer from Document 1. Since Document 1's answer is about the product being AI and machine learning solutions, and the FAQs section's answer is the product name, the answer should be IntelliBot. \n",
      "\n",
      "I don't see any other documents that mention the product in the FAQs. So the answer is correct.\n",
      "</think>\n",
      "\n",
      "The product is IntelliBot, an AI-driven chatbot designed for customer service automation and engagement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {response['answer']}\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "072230c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content within <think> tags:\n",
      "Okay, let me see. The user is asking for the answer to the question \"What is your product?\" based on the provided context. \n",
      "\n",
      "Looking at the documents, Document 1 is the company overview. In that document, there's a mention: \"TechTrend Innovations is a leading technology company specializing in artificial intelligence and machine learning solutions.\" So the product here is AI and machine learning solutions.\n",
      "\n",
      "Then there's Document 4, which is the product specifications for IntelliBot Chatbot. The product name is IntelliBot, version 2.1, released March 10, 2025, and the key features include AI-driven chatbots for customer service. The system requirements and pricing are listed there too. \n",
      "\n",
      "So, the product mentioned in the FAQs is the IntelliBot chatbot. The answer should be that the product is IntelliBot, which is an AI-driven chatbot. \n",
      "\n",
      "I need to make sure there's no conflicting information. The FAQs section only includes the question and the answer from Document 1. Since Document 1's answer is about the product being AI and machine learning solutions, and the FAQs section's answer is the product name, the answer should be IntelliBot. \n",
      "\n",
      "I don't see any other documents that mention the product in the FAQs. So the answer is correct.\n",
      "Answer without <think> tags:\n",
      "The product is IntelliBot, an AI-driven chatbot designed for customer service automation and engagement.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Extract content within <think> and </think>\n",
    "think_content = re.search(r\"<think>(.*?)</think>\", response['answer'], re.DOTALL)\n",
    "if think_content:\n",
    "    print(\"Content within <think> tags:\")\n",
    "    print(think_content.group(1).strip())\n",
    "else:\n",
    "    print(\"No content found within <think> tags.\")\n",
    "\n",
    "# Remove content within <think> and </think> from the answer\n",
    "cleaned_answer = re.sub(r\"<think>(.*?)</think>\", \"\", response['answer'], flags=re.DOTALL).strip()\n",
    "if cleaned_answer:\n",
    "    print(\"Answer without <think> tags:\")\n",
    "    print(cleaned_answer)\n",
    "else:\n",
    "    print(\"No content found outside <think> tags.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebd554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sources:\")\n",
    "for i, source in enumerate(response.get(\"sources\", [])):\n",
    "    print(f\"Source {i+1} (score: {source['score']:.4f}):\")\n",
    "    print(f\"From: {source['source']}\")\n",
    "    print(f\"Content preview: {source['content'][:150]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
